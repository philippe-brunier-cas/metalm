---
title: "Data Modeling "
subtitle: "metalm"
author: "pbrunier@cogne.com"
version: 1.0 
date: "`r Sys.Date()`"
output:
  rmdformats::readthedown:
    css: ./custom.css
    df_print: paged
    gallery: no
    highlight: default
    html_document: null
    lightbox: yes
    number_sections: yes
    self_contained: yes
    thumbnails: no
editor_options:
  chunk_output_type: console
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE,
                      warning = FALSE,
                      message = FALSE,
                      error = FALSE,
                      fig.height = 7,
                      fig.width = 10,
                      collapse = TRUE,
                      cols.print=20)
rm(list = ls(all.names = TRUE))

require(dplyr)
require(rlang)
require(kableExtra)
require(readr)
require(ggplot2)
require(rpart)
require(reticulate)
require(rpart.plot)
source('~/dev/metalm/R/function.R')
```

# esame delle variabili (input VS output)

leggiamo i dati dal file .pkl

```{python loading_dati}
import pandas as pd

df0 = pd.read_pickle('/data/metalm/pickle/dbmm.pkl')

features0 = ['Fz_iniziale','Fz_peak','dF_perc', 'C', 'S', 'P', 'Si', 'Mn', 'Cr', 'Ni', 'Mo','Cu', 'Sn', 'Al', 'V', 'W', 'Co',
'Ti', 'Nb', 'B', 'Ca', 'N2', 'O2 ','k2_oa', 'k2_os', 'k2_og', 'rm', 'rp02']

features = ['tempo_usura','C', 'S', 'P', 'Si', 'Mn', 'Cr', 'Ni', 'Mo','Cu', 'Sn', 'Al', 'V', 'W', 'Co',
'Ti', 'Nb', 'B', 'Ca', 'N2', 'O2 ']
       
df = df0[features]
```
## tempo_usura VS variabili risposta

```{r correlazioni,results='asis'}
dati <- py$df0
myfeatures <- py$features0

response <- 'tempo_usura'

vars <- dati %>% 
  select(myfeatures) %>%
  names()
n <- length(vars)

for (i in seq_len(n)) {
  var_i <- vars[i]
  pl <- ggplot(dati) +
    geom_point(aes(!!sym(var_i),!!sym(response))) +
    geom_smooth(
      aes(!!sym(var_i),!!sym(response)),
      color = 'red',
      se = FALSE,
      method = 'loess'
    ) +
    geom_smooth(
      aes(!!sym(var_i),!!sym(response)),
      color = 'green',
      se = FALSE,
      method = 'lm'
    )
  title <- paste('## Variabile: ', var_i)
  cat(title, '\n')
  print(pl)
  cat('<p>') #necessario per terminare paragrafo
}
```

# decision tree
```{r decision_tree}
dfr <- py$df

fm <- rpart(formula=tempo_usura ~ .,
            data = dfr,
            method = 'anova',
            cp = 0.02)

rpart.plot(fm,
           extra=101)

```

# correlation matrix 
```{python correlaion}

import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
#import './../py/metalm_func' as mm 

corr = df.corr()

dfcorr = pd.DataFrame()
dfcorr['correlation'] = corr.tempo_usura
dfcorr['absolute'] = np.abs(corr.tempo_usura)

dfcorr = dfcorr.sort_values('absolute')

print(dfcorr.correlation[-6:-1])

#isolo le prime n veriabili
dcorr = dfcorr.iloc[-6:-1,:]


```

```{r plot table}
tabella <- py$dcorr
cas_kable(tabella, caption='database')
```

# regressione lineare

## con solo F0
```{python linear_model}

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression

plt.style.use('seaborn')

feature_x = 'Fz_iniziale'
feature_y = 'tempo_usura'

x = df0[feature_x].values
y = df0[feature_y].values

# reshape dei dati necessario per il fit
# perche al momento sono array(L,) e devono diventare array(L,1)

   
L = len(x)
x = x.reshape(L,1)
y = y.reshape(L,1)

model = LinearRegression().fit(x, y)
r_sq = model.score(x, y)
print('modello lineare')
#print(feature_x)
#print(feature_y)
# print('y = m*x + q')
# print('m = %1.3f' %model.slope)
print('R2: \t %1.2f' %r_sq)

print(model.coef_)
print(model.intercept_)

plt.figure(figsize=(10,5))
plt.plot(x,y,'o',label='data')

xn = np.arange(x.min(),x.max(),step=(x.max()-x.min())/100 )
yn = model.predict(xn.reshape(len(xn),1))

plt.plot(xn,yn,'--',label='fitted line')
plt.text(x.min(),y.min()+.5,'r2 = %1.2f' %r_sq)
plt.legend(loc='best')
plt.title('Linear Model\n%s vs %s' %(feature_x,feature_y))
plt.xlabel(feature_x)
plt.ylabel(feature_y)
plt.show()

plt.style.use('default')

```

## chmica vs tempo usura - modello1

in questo modello consideriamo le prime 5 variabili della matrice di correlazione (in valore assoluto)

```{python multilinear_model}
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression
import statsmodels.api as sm

plt.style.use('seaborn')

feature_x = list(dcorr.index)
feature_y = 'tempo_usura'

x = df[feature_x].values
y = df[feature_y].values

# reshape dei dati necessario per il fit
# perche al momento sono array(L,) e devono diventare array(L,1)

model = LinearRegression().fit(x, y)
r_sq = model.score(x, y)
print('modello lineare a pi첫 variabili')
print('x = ' ,feature_x)
print('y = ' ,feature_y)

# print('y = m*x + q')
# print('m = %1.3f' %model.slope)
print('R2: \t %1.2f' %r_sq)
print('coefficents = ' ,model.coef_)
print('intercept = ', model.intercept_)
r,c = x.shape

valori = np.zeros(r)
for i in range(0,r):
  for j in range(0,c):
      valori[i] += np.asarray(df[feature_x[j]])[i]*model.coef_[j]
  valori[i] += model.intercept_
  
xn = np.arange(df[feature_y].min(),df[feature_y].max(),(df[feature_y].max()-df[feature_y].min())/100)
plt.figure(figsize=(8,6))
plt.title('fitting goodness\n%s' %feature_y)
plt.plot(valori,df[feature_y],'v',label='simulated')
plt.plot(xn,xn,'--',label='perfect fit')
plt.text(xn.min(),yn.min()+.5,'r2 = %1.2f' %r_sq)
# for i in range(0,len(valori)):
#     plt.text(valori[i],df[feature_y][i]+0.05,str(df.sk[i]))
plt.xlabel('predicted')
plt.ylabel('real')
plt.legend(loc='best')
plt.show()

plt.style.use('default')
```

### statistica descrittiva del modello1

```{python stats}

x = df[feature_x].values
y = df['tempo_usura'].values

x = sm.add_constant(x)

model_sm = sm.OLS(y,x)
results = model_sm.fit()
print(results.summary())
    
```
## chmica vs tempo usura - modello1 con trasformate

logaritmo

```{python model1_transformed}
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression
import statsmodels.api as sm

plt.style.use('seaborn')

feature_x = list(dcorr.index)
feature_y = 'tempo_usura_log'
df['tempo_usura_log'] = np.log(df.tempo_usura)

x = df[feature_x].values
y = df[feature_y].values

# reshape dei dati necessario per il fit
# perche al momento sono array(L,) e devono diventare array(L,1)

model = LinearRegression().fit(x, y)
r_sq = model.score(x, y)
print('modello lineare a pi첫 variabili')
print('x = ',feature_x)
print('y = ' ,feature_y)

# print('y = m*x + q')
# print('m = %1.3f' %model.slope)
print('R2: \t %1.2f' %r_sq)
print('coefficents = ' ,model.coef_)
print('intercept = ', model.intercept_)
r,c = x.shape

valori = np.zeros(r)
for i in range(0,r):
  for j in range(0,c):
      valori[i] += np.asarray(df[feature_x[j]])[i]*model.coef_[j]
  valori[i] += model.intercept_
  
xn = np.arange(df[feature_y].min(),df[feature_y].max(),(df[feature_y].max()-df[feature_y].min())/100)

plt.figure(figsize=(8,6))
plt.title('fitting goodness\n%s' %feature_y)
plt.plot(valori,df[feature_y],'v',label='simulated')
plt.plot(xn,xn,'--',label='perfect fit')
plt.text(xn.min(),yn.min()+.5,'r2 = %1.2f' %r_sq)
# for i in range(0,len(valori)):
#     plt.text(valori[i],df[feature_y][i]+0.05,str(df.sk[i]))
plt.xlabel('predicted')
plt.ylabel('real')
plt.legend(loc='best')
plt.show()

plt.style.use('default')

```

### statistica descrittiva del modello1

```{python stats_transf}

x = df[feature_x].values
y = df['tempo_usura_log'].values

x = sm.add_constant(x)

model_sm = sm.OLS(y,x)
results = model_sm.fit()
print(results.summary())
    
```

## chimica vs tempo usura - variabili scalate

```{python multilinear_model_scaled}
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression
import statsmodels.api as sm

plt.style.use('seaborn')

feature_x = list(dcorr.index)
feature_y = 'tempo_usura'

x = df[feature_x].values
y = df[feature_y].values

# reshape dei dati necessario per il fit
# perche al momento sono array(L,) e devono diventare array(L,1)

model = LinearRegression().fit(x, y)
r_sq = model.score(x, y)
print('modello lineare a pi첫 variabili')
print('x = ',feature_x)
print('y = ' ,feature_y)

# print('y = m*x + q')
# print('m = %1.3f' %model.slope)
print('R2: \t %1.2f' %r_sq)
print('coefficents = ' ,model.coef_)
print('intercept = ', model.intercept_)
r,c = x.shape

valori = np.zeros(r)
for i in range(0,r):
  for j in range(0,c):
      valori[i] += np.asarray(df[feature_x[j]])[i]*model.coef_[j]
  valori[i] += model.intercept_
  
xn = np.arange(df[feature_y].min(),df[feature_y].max(),(df[feature_y].max()-df[feature_y].min())/100)
plt.figure(figsize=(8,6))
plt.title('fitting goodness\n%s' %feature_y)
plt.plot(valori,df[feature_y],'v',label='simulated')
plt.plot(xn,xn,'--',label='perfect fit')
plt.text(xn.min(),yn.min()+.5,'r2 = %1.2f' %r_sq)
# for i in range(0,len(valori)):
#     plt.text(valori[i],df[feature_y][i]+0.05,str(df.sk[i]))
plt.xlabel('predicted')
plt.ylabel('real')
plt.legend(loc='best')
plt.show()

plt.style.use('default')
```

## chimica VS tempo usura - modello2

in questo modello consideriamo O2 ed altre variabili che **ci sembra** possano avere un influenza 

```{python multilinear_model2}
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression
import statsmodels.api as sm

plt.style.use('seaborn')

feature_x = ['Si','O2 ','C','Cr','Cu']
feature_y = 'tempo_usura'

x = df[feature_x].values
y = df[feature_y].values

# reshape dei dati necessario per il fit
# perche al momento sono array(L,) e devono diventare array(L,1)

model = LinearRegression().fit(x, y)
r_sq = model.score(x, y)
print('modello lineare a pi첫 variabili')
print('x = ',feature_x)
print('y = ' ,feature_y)

# print('y = m*x + q')
# print('m = %1.3f' %model.slope)
print('R2: \t %1.2f' %r_sq)
print('coefficents = ' ,model.coef_)
print('intercept = ', model.intercept_)
r,c = x.shape

valori = np.zeros(r)
for i in range(0,r):
  for j in range(0,c):
      valori[i] += np.asarray(df[feature_x[j]])[i]*model.coef_[j]
  valori[i] += model.intercept_
  
xn = np.arange(df[feature_y].min(),df[feature_y].max(),(df[feature_y].max()-df[feature_y].min())/100)
plt.figure(figsize=(8,6))
plt.title('fitting goodness\n%s' %feature_y)
plt.plot(valori,df[feature_y],'v',label='simulated')
plt.plot(xn,xn,'--',label='perfect fit')
plt.text(xn.min(),yn.min()+.5,'r2 = %1.2f' %r_sq)
# for i in range(0,len(valori)):
#     plt.text(valori[i],df[feature_y][i]+0.05,str(df.sk[i]))
plt.xlabel('predicted')
plt.ylabel('real')
plt.legend(loc='best')
plt.show()

plt.style.use('default')
```

### statistica descrittiva del modello2

```{python stats2}
print('x -> ',feature_x)
print('y ->',feature_y)

x = df0[feature_x].values
y = df0[feature_y].values

x = sm.add_constant(x)

model_sm = sm.OLS(y,x)
results = model_sm.fit()
print(results.summary())

```