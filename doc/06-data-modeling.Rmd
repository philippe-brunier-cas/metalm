---
title: "Data Modeling "
subtitle: "metalm"
author: "pbrunier@cogne.com"
version: 1.0 
date: "`r Sys.Date()`"
output:
  rmdformats::readthedown:
    css: ./custom.css
    df_print: paged
    gallery: no
    highlight: default
    html_document: null
    lightbox: yes
    number_sections: yes
    self_contained: yes
    thumbnails: no
editor_options:
  chunk_output_type: console
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE,
                      warning = FALSE,
                      message = FALSE,
                      error = FALSE,
                      fig.height = 7,
                      fig.width = 10,
                      collapse = TRUE,
                      cols.print=20)
rm(list = ls(all.names = TRUE))

require(dplyr)
require(rlang)
require(kableExtra)
require(readr)
require(ggplot2)
require(rpart)
require(reticulate)
require(rpart.plot)
require(tidyr)
source('~/dev/metalm/R/function.R')
```

# esame delle variabili (input VS output)

leggiamo i dati dal file .pkl

```{python loading_dati}
import pandas as pd

df0 = pd.read_pickle('/data/metalm/pickle/dbmm.pkl')
df0 = df0[df0.materiale != 'F304LH']

features0 = ['Fz_iniziale','Fz_peak','dF_perc', 'C', 'S', 'P', 'Si', 'Mn', 'Cr', 'Ni', 'Mo','Cu', 'Sn', 'Al', 'V', 'W', 'Co',
'Ti', 'Nb', 'B', 'Ca', 'N2', 'O2 ','k2_oa', 'k2_os', 'k2_og', 'rm', 'rp02']

features = ['tempo_usura','C', 'S', 'P', 'Si', 'Mn', 'Cr', 'Ni', 'Mo','Cu', 'Sn', 'Al', 'V', 'W', 'Co',
'Ti', 'Nb', 'B', 'Ca', 'N2', 'O2 ']
       
df = df0[features]
```

# statistica descrittiva generale

```{r summary}

source_python('~/dev/metalm/py/read_pickle_file.py')
dati_prove <- read_pickle_file('/data/metalm/pickle/dbmm.pkl')

summary(dati_prove)

```


# decision tree
```{r decision_tree}
dfr <- py$df

fm <- rpart(formula=tempo_usura ~ .,
            data = dfr,
            method = 'anova',
            cp = 0.02)

rpart.plot(fm,
           extra=101)

```

# correlation matrix 
```{python correlaion}

import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
#import './../py/metalm_func' as mm 

corr = df.corr()

dfcorr = pd.DataFrame()
dfcorr['correlation'] = corr.tempo_usura
dfcorr['absolute'] = np.abs(corr.tempo_usura)

dfcorr = dfcorr.sort_values('absolute')

print(dfcorr.correlation[-6:-1])

#isolo le prime n veriabili
dcorr = dfcorr.iloc[-6:-1,:]


```

```{r plot table}
tabella <- py$dcorr
cas_kable(tabella, caption='database')
```


# regressione lineare

## con solo F0
```{python linear_model}

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression

plt.style.use('seaborn')

feature_x = 'Fz_iniziale'
feature_y = 'tempo_usura'

x = df0[feature_x].values
y = df0[feature_y].values

# reshape dei dati necessario per il fit
# perche al momento sono array(L,) e devono diventare array(L,1)

   
L = len(x)
x = x.reshape(L,1)
y = y.reshape(L,1)

model = LinearRegression().fit(x, y)
r_sq = model.score(x, y)
print('modello lineare')
#print(feature_x)
#print(feature_y)
# print('y = m*x + q')
# print('m = %1.3f' %model.slope)
print('R2: \t %1.2f' %r_sq)

print(model.coef_)
print(model.intercept_)

plt.figure(figsize=(10,5))
plt.plot(x,y,'o',label='data')

xn = np.arange(x.min(),x.max(),step=(x.max()-x.min())/100 )
yn = model.predict(xn.reshape(len(xn),1))

plt.plot(xn,yn,'--',label='fitted line')
plt.text(x.min(),y.min()+.5,'r2 = %1.2f' %r_sq)
plt.legend(loc='best')
plt.title('Linear Model\n%s vs %s' %(feature_x,feature_y))
plt.xlabel(feature_x)
plt.ylabel(feature_y)
plt.show()

plt.style.use('default')

```

## chmica vs tempo usura - modello1

in questo modello consideriamo le prime 5 variabili della matrice di correlazione (in valore assoluto)

```{python multilinear_model}
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression
import statsmodels.api as sm

plt.style.use('seaborn')

feature_x = list(dcorr.index)
feature_y = 'tempo_usura'

x = df[feature_x].values
y = df[feature_y].values

# reshape dei dati necessario per il fit
# perche al momento sono array(L,) e devono diventare array(L,1)

model = LinearRegression().fit(x, y)
r_sq = model.score(x, y)
print('modello lineare a più variabili')
print('x = ' ,feature_x)
print('y = ' ,feature_y)

# print('y = m*x + q')
# print('m = %1.3f' %model.slope)
print('R2: \t %1.2f' %r_sq)
print('coefficents = ' ,model.coef_)
print('intercept = ', model.intercept_)
r,c = x.shape

valori = np.zeros(r)
for i in range(0,r):
  for j in range(0,c):
      valori[i] += np.asarray(df[feature_x[j]])[i]*model.coef_[j]
  valori[i] += model.intercept_
  
xn = np.arange(df[feature_y].min(),df[feature_y].max(),(df[feature_y].max()-df[feature_y].min())/100)
plt.figure(figsize=(8,6))
plt.title('fitting goodness\n%s' %feature_y)
plt.plot(valori,df[feature_y],'v',label='simulated')
plt.plot(xn,xn,'--',label='perfect fit')
plt.text(xn.min(),yn.min()+.5,'r2 = %1.2f' %r_sq)
# for i in range(0,len(valori)):
#     plt.text(valori[i],df[feature_y][i]+0.05,str(df.sk[i]))
plt.xlabel('predicted')
plt.ylabel('real')
plt.legend(loc='best')
plt.show()

plt.style.use('default')
```

### statistica descrittiva del modello1

```{python stats}

x = df[feature_x].values
y = df['tempo_usura'].values

x = sm.add_constant(x)

model_sm = sm.OLS(y,x)
results = model_sm.fit()
print(results.summary())
    
```
## chmica vs tempo usura - modello1 con trasformate

logaritmo

```{python model1_transformed}
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression
import statsmodels.api as sm

plt.style.use('seaborn')

feature_x = list(dcorr.index)
feature_y = 'tempo_usura_log'
df['tempo_usura_log'] = np.log(df.tempo_usura)

x = df[feature_x].values
y = df[feature_y].values

# reshape dei dati necessario per il fit
# perche al momento sono array(L,) e devono diventare array(L,1)

model = LinearRegression().fit(x, y)
r_sq = model.score(x, y)
print('modello lineare a più variabili')
print('x = ',feature_x)
print('y = ' ,feature_y)

# print('y = m*x + q')
# print('m = %1.3f' %model.slope)
print('R2: \t %1.2f' %r_sq)
print('coefficents = ' ,model.coef_)
print('intercept = ', model.intercept_)
r,c = x.shape

valori = np.zeros(r)
for i in range(0,r):
  for j in range(0,c):
      valori[i] += np.asarray(df[feature_x[j]])[i]*model.coef_[j]
  valori[i] += model.intercept_
  
xn = np.arange(df[feature_y].min(),df[feature_y].max(),(df[feature_y].max()-df[feature_y].min())/100)

plt.figure(figsize=(8,6))
plt.title('fitting goodness\n%s' %feature_y)
plt.plot(valori,df[feature_y],'v',label='simulated')
plt.plot(xn,xn,'--',label='perfect fit')
plt.text(xn.min(),yn.min()+.5,'r2 = %1.2f' %r_sq)
# for i in range(0,len(valori)):
#     plt.text(valori[i],df[feature_y][i]+0.05,str(df.sk[i]))
plt.xlabel('predicted')
plt.ylabel('real')
plt.legend(loc='best')
plt.show()

plt.style.use('default')

```

### statistica descrittiva del modello1

```{python stats_transf}

x = df[feature_x].values
y = df['tempo_usura_log'].values

x = sm.add_constant(x)

model_sm = sm.OLS(y,x)
results = model_sm.fit()
print(results.summary())
    
```
# outliers

detection degli outliers con il seguente metodo:

+ scalatura dei dati, secondo la fomrula: $x_{scaled} = \frac{x - \tilde{x}}{\sigma_x}$ 

dove:

  + $x_{scaled}$ è la variabile scalata

  + $x$ è la variabile

  + $\tilde{x}$ è la media di x

  + $\sigma_x$ è la varianza

+ identificazione per valore assoluto: tutto ciò che cade fuori 2 (95% confidenza) viene considerato outlier

```{python outliers_with_scaling}
import pandas as pd

df = pd.read_pickle('/data/metalm/pickle/dbmm.pkl')
# features = ['tempo_usura','C', 'S', 'P', 'Si', 'Mn', 'Cr', 'Ni', 'Mo','Cu', 'Sn', 'Al', 'V', 'W', 'Co','Ti', 'Nb', 'B', 'Ca', 'N2', 'O2 ']

# target = features[0]
features = features[1:]

## creating dataframe scaled

dfs = pd.DataFrame()
for ff in features:
    try:
      media = df[ff].mean()
      std_dev = df[ff].std()
    except KeyError:
      pass
    
    
    dfs[ff] = (df[ff] - media) / std_dev

rs,cs = dfs.shape

# cleaning if abs(value) > n_sigma
# questo vuol dire non considerare ciò che va oltre i 3 sigma
n_sigma = 2
dfo = dfs
for ff in features:
    # pos = np.where(dfo[ff] < n_sigma)[0]
    dfo = dfo[dfo[ff] < n_sigma]

ro,co = dfo.shape

print('dropped %d outliers [%1.2f %s of data]' %((rs-ro),(rs-ro)/rs*100,'%'))

dfoo = df.loc[dfo.index,:]
dfoo.to_pickle('/data/metalm/pickle/dbmm_scaled.pkl')
```

## chimica vs tempo usura - variabili scalate

```{python multilinear_model_scaled}
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression
import statsmodels.api as sm

plt.style.use('seaborn')
features = ['tempo_usura', 'C', 'S', 'P', 'Si', 'Mn', 'Cr', 'Ni', 'Mo', 'Cu', 'Sn', 'Al', 'V', 'W', 'Co', 'Ti', 'Nb', 'B', 'Ca', 'N2', 'O2 ']
df = dfoo[features]
corr = df.corr()

dfcorr = pd.DataFrame()
dfcorr['correlation'] = corr.tempo_usura
dfcorr['absolute'] = np.abs(corr.tempo_usura)

dfcorr = dfcorr.sort_values('absolute')

print(dfcorr.correlation[-6:-1])

#isolo le prime n veriabili
dcorr = dfcorr.iloc[-6:-1,:]

```

```{r plot table scaled}
tabella <- py$dcorr
cas_kable(tabella, caption='database')
```

## chimica VS tempo usura - modello scalato

in questo modello consideriamo O2 ed altre variabili che sono state identificate dopo la pulizia 

```{python multilinear_model2}
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression
import statsmodels.api as sm

plt.style.use('seaborn')

feature_x = ['Si','O2 ','Ti','Cu','Cr']
feature_y = 'tempo_usura'

x = df[feature_x].values
y = df[feature_y].values

# reshape dei dati necessario per il fit
# perche al momento sono array(L,) e devono diventare array(L,1)

model = LinearRegression().fit(x, y)
r_sq = model.score(x, y)
print('modello lineare a più variabili')
print('x = ',feature_x)
print('y = ' ,feature_y)

# print('y = m*x + q')
# print('m = %1.3f' %model.slope)
print('R2: \t %1.2f' %r_sq)
print('coefficents = ' ,model.coef_)
print('intercept = ', model.intercept_)
r,c = x.shape

valori = np.zeros(r)
for i in range(0,r):
  for j in range(0,c):
      valori[i] += np.asarray(df[feature_x[j]])[i]*model.coef_[j]
  valori[i] += model.intercept_
  
xn = np.arange(df[feature_y].min(),df[feature_y].max(),(df[feature_y].max()-df[feature_y].min())/100)
plt.figure(figsize=(8,6))
plt.title('fitting goodness\n%s' %feature_y)
plt.plot(valori,df[feature_y],'v',label='simulated')
plt.plot(xn,xn,'--',label='perfect fit')
plt.text(xn.min(),yn.min()+.5,'r2 = %1.2f' %r_sq)
# for i in range(0,len(valori)):
#     plt.text(valori[i],df[feature_y][i]+0.05,str(df.sk[i]))
plt.xlabel('predicted')
plt.ylabel('real')
plt.legend(loc='best')
plt.show()

plt.style.use('default')
```

### statistica descrittiva del modello2

```{python stats2}
print('x -> ',feature_x)
print('y ->',feature_y)

x = df0[feature_x].values
y = df0[feature_y].values

x = sm.add_constant(x)

model_sm = sm.OLS(y,x)
results = model_sm.fit()
print(results.summary())

```

# regressione non lineare

```{r non_linear_regression}

source_python('~/dev/metalm/py/read_pickle_file.py')
dati_prove <- read_pickle_file('/data/metalm/pickle/dbmm.pkl')

#modello con interazioni

modello <- lm ( tempo_usura  ~  Si + `O2 ` + Ti + Cu +
                  Si*Si + Si*`O2 ` + 
                  `O2 `*`O2 ` + `O2 `*Ti + 
                  Ti*Ti + Cu*Cu
                  , data = dati_prove)

print(modello$coefficients)
cor(dati_prove$tempo_usura,predict(modello))

# plots residui
par(mfrow = c(2, 2))
plot(modello)
par(mfrow = c(1, 1))

summary(modello)
```

```{r non_linear_regression with log}

source_python('~/dev/metalm/py/read_pickle_file.py')
dati_prove <- read_pickle_file('/data/metalm/pickle/dbmm.pkl')

dati_prove$tempo_usura_log <- log(dati_prove$tempo_usura)
#modello con interazioni

modello_log <- lm ( tempo_usura_log  ~  Si + `O2 ` + Ti + Cu + Cr +
                  Si*`O2 ` + `O2 `*Ti + `O2 `*Cr + `O2 `*Cu 
                  , data = dati_prove)

print(modello_log$coefficients)
cor(dati_prove$tempo_usura_log,predict(modello_log))

# plots residui
par(mfrow = c(2, 2))
plot(modello_log)
par(mfrow = c(1, 1))

summary(modello_log)
```
